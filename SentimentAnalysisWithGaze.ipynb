{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nF6rmebuc89cCVvwLTuXSh5Y_wf8vGKV",
      "authorship_tag": "ABX9TyM6GRykAYguwCQNNaF3A/xU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilhenAlbertoHM/SentimentAnalysisWithGaze/blob/main/SentimentAnalysisWithGaze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jsyYa3AGAkXb"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import libraries"
      ],
      "metadata": {
        "id": "YM18MXS-E9DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bur9HGQREYv2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Organize data into time series"
      ],
      "metadata": {
        "id": "UfaAKqyyAqmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "organize_into_time_series: Organizes the data into time series, displayed in two .npy files,\n",
        "                           for features and labels\n",
        "Param: n_steps - The number of time steps (30 fps from a video)\n",
        "Returns: None.\n",
        "\"\"\"\n",
        "def organize_into_time_series(n_steps=30):\n",
        "    # Load the data.\n",
        "    data = np.genfromtxt(\"csv_files/gaze_rotation_and_scores_data_cleaned.csv\", delimiter=\",\")\n",
        "\n",
        "    # For NaN values, replace them with the average of the column.\n",
        "    mean_values = np.nanmean(data, axis=0)\n",
        "    nan_indices = np.isnan(data)\n",
        "    data[nan_indices] = np.take(mean_values, np.where(nan_indices)[1])\n",
        "\n",
        "    # Get the window size by the start and end indices\n",
        "    start_index = n_steps\n",
        "    end_index = data.shape[0]\n",
        "\n",
        "    # Append features and labels in specific time steps.\n",
        "    features = []\n",
        "    labels = []\n",
        "    feature_size = data.shape[1] - 5\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i - n_steps, i)\n",
        "\n",
        "        # Features are the first 13 columns and labels are the last 5 columns.\n",
        "        labels.append(data[indices[0]][feature_size:])\n",
        "        features.append(np.delete(data[indices], range(feature_size, data.shape[1]), axis=1))\n",
        "\n",
        "    # Save the data and labels in .npy files.\n",
        "    np.save(\"features_\" + str(n_steps) + \"_steps\" + \".npy\", np.array(features))\n",
        "    np.save(\"labels_\" + str(n_steps) + \"_steps\" + \".npy\", np.array(labels))"
      ],
      "metadata": {
        "id": "ZhAYBIuCE6Jp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load data by the offset or gap between time steps"
      ],
      "metadata": {
        "id": "H2g2GZ6JFMHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "load_data: Loads organized time series data; features.npy and labels.npy\n",
        "Param: offset - The offset or gap between time steps\n",
        "Returns: Data regarding features and labels as a pair\n",
        "\"\"\"\n",
        "def load_data(features_path, labels_path, offset=1):\n",
        "    file_features = \"features_60_steps.npy\"\n",
        "    file_labels = \"labels_60_steps.npy\"\n",
        "    features = np.load(file_features)\n",
        "    labels = np.load(file_labels)\n",
        "\n",
        "    # Display shapes\n",
        "    print(f\"Features shape: {features.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    return features[::offset], labels[::offset]"
      ],
      "metadata": {
        "id": "gMhBEY3LFKxT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create the model"
      ],
      "metadata": {
        "id": "h5SFU9MCFm-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "create_cnn_model: Creates a Sequential CNN model with given number of time steps,\n",
        "                  features, and kernel and pool sizes\n",
        "Params: n_timesteps - Number of time steps\n",
        "        n_features - Number of features\n",
        "                     (e.g., coordinates for eye gaze in the x, y, and z axes)\n",
        "        n_filters - Number of filters\n",
        "        kernel_size - Size of kernel for Conv1D\n",
        "        pool_size - Size of pool for Conv1D\n",
        "Returns: Sequential CNN model\n",
        "\"\"\"\n",
        "def create_cnn_model(n_timesteps, n_features, n_filters=128, kernel_size=15, pool_size=2):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(n_timesteps, n_features)))\n",
        "    model.add(Conv1D(filters=n_filters, kernel_size=kernel_size, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=5, activation=\"relu\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "DAm0jeo7Fb8b"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Provide data visualization"
      ],
      "metadata": {
        "id": "cX3k-UjlGzVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "plot_train_test_loss: Plots training and testing loss, both MSE and MAE\n",
        "Params: history - the saved history over the epochs within the model\n",
        "Returns: None\n",
        "\"\"\"\n",
        "def plot_train_test_loss(history):\n",
        "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"test_loss\")\n",
        "    plt.plot(history.history[\"mae\"], label=\"train_mae\")\n",
        "    plt.plot(history.history[\"val_mae\"], label=\"test_mae\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M-itH3bWGyit"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train model and perform evaluation metrics"
      ],
      "metadata": {
        "id": "MkJ793jFIDMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "main: The main function\n",
        "\"\"\"\n",
        "def main():\n",
        "    # Organize data into time series by creating two .npy files\n",
        "    # for data and labels. After, load data into x and y.\n",
        "    # organize_into_time_series(n_steps=60)\n",
        "    features_path = \"features_and_labels_npy/features_60_steps.npy\"\n",
        "    labels_path = \"features_and_labels_npy/labels_60_steps.npy\"\n",
        "    x, y = load_data(features_path, labels_path, offset=5)\n",
        "\n",
        "    # Print the shapes.\n",
        "    print(f\"X shape: {x.shape}\")\n",
        "    print(f\"Y shape: {y.shape}\")\n",
        "\n",
        "    # Split the data into training and testing sets.\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Further split the training set into training and validation sets.\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Scale the data.\n",
        "    scaler = MinMaxScaler()\n",
        "    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
        "    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n",
        "    x_val = scaler.transform(x_val.reshape(-1, x_val.shape[-1])).reshape(x_val.shape)\n",
        "\n",
        "    # Get the timesteps (30), number of features (13), and number of labels (5).\n",
        "    n_timesteps, n_features = x.shape[1], x.shape[2]\n",
        "    n_labels = y.shape[1]\n",
        "\n",
        "    # Create the train and test datasets.\n",
        "    buffer_size = 60_000\n",
        "    batch_size = 16\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size).batch(batch_size).repeat()\n",
        "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "    # Create the model.\n",
        "    model = create_cnn_model(n_timesteps, n_features)\n",
        "    model.summary()\n",
        "\n",
        "    # Compile and train the model.\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
        "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
        "    history = model.fit(train_data,\n",
        "                        steps_per_epoch=len(x_train) // batch_size,\n",
        "                        epochs=20,\n",
        "                        validation_data=val_data,\n",
        "                        validation_steps=len(x_val) // batch_size,\n",
        "                        callbacks=[es])\n",
        "    # Evaluate the model\n",
        "    mse, mae = model.evaluate(x_test, y_test)\n",
        "    print(f\"MSE: {mse}\")\n",
        "    print(f\"MAE: {mae}\")\n",
        "\n",
        "    # Predict the labels.\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_diff = np.abs(np.subtract(y_pred, y_test))\n",
        "    print(f\"y_pred:\\n {y_pred}\")\n",
        "    print(f\"y_test:\\n {y_test}\")\n",
        "    print(f\"y_pred - y_test:\\n {y_diff}\")\n",
        "\n",
        "    # Plot training and test loss.\n",
        "    plot_train_test_loss(history)\n",
        "\n",
        "    # Save the model.\n",
        "    model.save(\"cnn_model\")"
      ],
      "metadata": {
        "id": "MOBhZIGvICny"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#   main()"
      ],
      "metadata": {
        "id": "C8_qT5fPKBNr"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}